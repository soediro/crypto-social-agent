{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 1: Import and setup",
   "id": "d3e3a8c16ba215b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\nfrom datasets import Dataset\nimport torch\n\nprint(\"üöÄ Starting Hello World Fine-Tuning!\")\n\n# Check CUDA availability and setup device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üî• Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"üì± CUDA Device: {torch.cuda.get_device_name(0)}\")\n    print(f\"üß† CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n\nmodel_name = \"facebook/opt-350m\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# Use half precision if CUDA is available for memory efficiency\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n    torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n)\nmodel = model.to(device)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"‚úÖ Loaded model on: {model.device}\")\nprint(f\"üìä Model parameters: {model.num_parameters():,}\")",
   "id": "3df2ff85a9baa1bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "print(\"üöÄ Starting Hello World Fine-Tuning!\")\n"
   ],
   "id": "b508ac19765e3f99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 2: Load a TINY model (not even TinyLlama - something smaller for quick testing)",
   "id": "b53d697fc804de8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_name = \"facebook/opt-350m\"  # Even smaller than TinyLlama for speed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Loaded model: {model_name}\")\n",
    "print(f\"üìä Model parameters: {model.num_parameters():,}\")"
   ],
   "id": "807d86a2a4037fd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cell 3: Create SUPER simple training data\n",
    "# Goal: Teach the model to complete crypto-related sentences"
   ],
   "id": "e9085a54b4f10cfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_data = [\n",
    "    \"When Bitcoin price goes up, the market sentiment is bullish.\",\n",
    "    \"If a project has a rug pull, investors will lose money.\",\n",
    "    \"High trading volume usually indicates strong market interest.\",\n",
    "    \"When fear dominates the market, prices tend to drop.\",\n",
    "    \"A successful token launch often leads to price appreciation.\",\n",
    "    \"Security audits are important for smart contract safety.\",\n",
    "    \"Market capitalization reflects the total value of a cryptocurrency.\",\n",
    "    \"Liquidity pools enable decentralized trading on DEXs.\",\n",
    "    \"Whale movements can significantly impact token prices.\",\n",
    "    \"Technical analysis helps predict short-term price movements.\"\n",
    "]\n",
    "\n",
    "# Convert to dataset format\n",
    "dataset = Dataset.from_dict({\"text\": training_data})\n",
    "print(f\"üìà Training samples: {len(dataset)}\")"
   ],
   "id": "bb0aab42a5bcdab7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 4: Tokenize the data",
   "id": "91180b490ff89949"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,  # Keep it short for quick training\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(\"‚úÖ Data tokenized!\")"
   ],
   "id": "8c843fa8ba4223f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 5: Set up training arguments (MINIMAL for quick testing)",
   "id": "fab592c1219cd061"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "training_args = TrainingArguments(\n    output_dir=\"./models/fine_tuned/hello_world\",\n    num_train_epochs=3,           # Very short training\n    per_device_train_batch_size=1, # Reduced for GTX 1050 Mobile memory\n    gradient_accumulation_steps=2, # Compensate for smaller batch size\n    logging_steps=1,              # See progress immediately\n    save_steps=10,\n    remove_unused_columns=False,\n    fp16=device.type == \"cuda\",   # Enable half precision if using CUDA\n    dataloader_pin_memory=False,  # Reduce memory usage\n    report_to=[],                 # Disable wandb/tensorboard logging\n)",
   "id": "74d95ecb0d7f49f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 6: Create trainer",
   "id": "5d14be081ce11774"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ],
   "id": "a8d4b401fb33d0d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 7: Train! (This should take 1-3 minutes)",
   "id": "b28bdc69d5550573"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 8: Test the fine-tuned model",
   "id": "ea06c3e4f4ccaf8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"üèãÔ∏è Starting training...\")\n\n# Check for CUDA compatibility issues\ntry:\n    # Test a simple CUDA operation first\n    if device.type == \"cuda\":\n        test_tensor = torch.randn(10, 10).to(device)\n        _ = test_tensor @ test_tensor.T  # Simple matrix multiplication test\n        print(\"‚úÖ CUDA compatibility test passed\")\n    \n    trainer.train()\n    print(\"üéâ Training completed!\")\n    \nexcept RuntimeError as e:\n    if \"CUDA\" in str(e) and \"kernel image\" in str(e):\n        print(\"‚ö†Ô∏è  CUDA compatibility issue detected (likely GTX 1050 compute capability)\")\n        print(\"üîÑ Switching to CPU training...\")\n        \n        # Move model and data to CPU\n        model = model.cpu()\n        device = torch.device(\"cpu\")\n        \n        # Update training arguments for CPU\n        training_args_cpu = TrainingArguments(\n            output_dir=\"./models/fine_tuned/hello_world\",\n            num_train_epochs=3,\n            per_device_train_batch_size=1,\n            gradient_accumulation_steps=2,\n            logging_steps=1,\n            save_steps=10,\n            remove_unused_columns=False,\n            fp16=False,  # Disable FP16 for CPU\n            dataloader_pin_memory=False,\n            report_to=[],\n        )\n        \n        # Create new trainer with CPU settings\n        trainer_cpu = Trainer(\n            model=model,\n            args=training_args_cpu,\n            train_dataset=tokenized_dataset,\n        )\n        \n        print(\"üîÑ Starting CPU training...\")\n        trainer_cpu.train()\n        print(\"üéâ CPU Training completed!\")\n        \n        # Update trainer reference for later cells\n        trainer = trainer_cpu\n        \n    else:\n        print(f\"‚ùå Unexpected error during training: {e}\")\n        raise e",
   "id": "3bfcfad69cbae361",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Cell 8: Test the fine-tuned model\ntest_prompts = [\n    \"When Ethereum price goes up,\",\n    \"If a project has good fundamentals,\",\n    \"High trading volume indicates\"\n]\n\nprint(\"\\nüß™ Testing fine-tuned model:\")\nfor prompt in test_prompts:\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    # Generate text with proper device handling\n    outputs = model.generate(\n        inputs.input_ids,\n        max_length=30,\n        num_return_sequences=1,\n        temperature=0.7,\n        pad_token_id=tokenizer.eos_token_id,\n        do_sample=True\n    )\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Prompt: {prompt}\")\n    print(f\"Completion: {generated_text}\")\n    print(\"-\" * 50)",
   "id": "111f995fbeadaab4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
