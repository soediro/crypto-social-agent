{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 1: Import and setup",
   "id": "d3e3a8c16ba215b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# CRITICAL: GTX 1050 CUDA Compatibility Setup\n# Run this cell, then RESTART KERNEL, then continue\n\nimport os\nimport sys\n\n# Set environment variables BEFORE any PyTorch imports\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Hide all CUDA devices completely\nos.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"   # Disable CUDA device-side assertions  \nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\" # Make CUDA calls synchronous\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:0\"  # Disable CUDA memory caching\n\nprint(\"üö® CRITICAL STEP FOR GTX 1050:\")\nprint(\"=\" * 50)\nprint(\"1. ‚úÖ Environment variables are now set\")\nprint(\"2. üîÑ Go to: Kernel ‚Üí Restart Kernel (keep outputs)\")\nprint(\"3. ‚ñ∂Ô∏è  After restart, run the next cell to continue\")\nprint(\"4. ‚ö†Ô∏è  DO NOT skip the restart - it's essential!\")\nprint(\"=\" * 50)\n\nprint(\"\\nüîß Environment configured:\")\nfor key in [\"CUDA_VISIBLE_DEVICES\", \"TORCH_USE_CUDA_DSA\", \"CUDA_LAUNCH_BLOCKING\", \"PYTORCH_CUDA_ALLOC_CONF\"]:\n    print(f\"   {key} = '{os.environ.get(key, 'NOT_SET')}'\")\n    \nprint(\"\\nüí° These settings will force CPU-only operation and avoid GTX 1050 CUDA kernel issues.\")",
   "id": "3df2ff85a9baa1bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# STEP 3: Load Model with Explicit CPU-Only Configuration\n\nmodel_name = \"facebook/opt-350m\"\nprint(f\"üì• Loading {model_name} with CPU-only configuration...\")\n\n# Load tokenizer\nprint(\"   Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load model with explicit CPU device mapping and no CUDA optimizations\nprint(\"   Loading model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n    torch_dtype=torch.float32,      # Use float32 for CPU\n    device_map=None,                # No device mapping (defaults to CPU)\n    low_cpu_mem_usage=False,        # Disable to avoid any CUDA optimizations\n)\n\n# Explicitly move model to CPU and verify\nmodel = model.to('cpu')\ndevice = torch.device('cpu')\n\n# Configure tokenizer\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"‚úÖ Model loaded successfully!\")\nprint(f\"üñ•Ô∏è  Model device: {next(model.parameters()).device}\")\nprint(f\"üìä Model parameters: {model.num_parameters():,}\")\nprint(f\"üîß Model dtype: {next(model.parameters()).dtype}\")\n\n# Verify no tensors are on CUDA\ncuda_tensors = 0\nfor name, param in model.named_parameters():\n    if param.device.type == 'cuda':\n        cuda_tensors += 1\n        print(f\"   ‚ö†Ô∏è  Found CUDA tensor: {name} on {param.device}\")\n\nif cuda_tensors == 0:\n    print(\"‚úÖ All model parameters confirmed on CPU\")\nelse:\n    print(f\"‚ùå Found {cuda_tensors} parameters on CUDA - this will cause issues\")",
   "id": "b508ac19765e3f99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 2: Load a TINY model (not even TinyLlama - something smaller for quick testing)",
   "id": "b53d697fc804de8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# STEP 2: Clean CPU-Only Setup (run AFTER kernel restart)\n\n# Verify environment variables are set\nimport os\nprint(\"üîç Checking environment variables:\")\nenv_vars = {\n    \"CUDA_VISIBLE_DEVICES\": os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"NOT_SET\"),\n    \"TORCH_USE_CUDA_DSA\": os.environ.get(\"TORCH_USE_CUDA_DSA\", \"NOT_SET\"), \n    \"CUDA_LAUNCH_BLOCKING\": os.environ.get(\"CUDA_LAUNCH_BLOCKING\", \"NOT_SET\"),\n    \"PYTORCH_CUDA_ALLOC_CONF\": os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\", \"NOT_SET\")\n}\n\nfor key, value in env_vars.items():\n    status = \"‚úÖ\" if value != \"NOT_SET\" else \"‚ùå\"\n    print(f\"   {status} {key} = '{value}'\")\n\n# If environment variables aren't set, set them now (fallback)\nif os.environ.get(\"CUDA_VISIBLE_DEVICES\") is None:\n    print(\"\\n‚ö†Ô∏è  Environment variables not found - setting them now:\")\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n    os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\" \n    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:0\"\n    print(\"   Environment variables set as fallback\")\n\n# Now import PyTorch with CPU-only configuration\nprint(\"\\nüì¶ Importing PyTorch with CPU-only configuration...\")\nimport torch\nimport torch.nn as nn\n\n# Verify PyTorch sees no CUDA devices\nprint(f\"üîç CUDA available: {torch.cuda.is_available()}\")\nprint(f\"üîç CUDA device count: {torch.cuda.device_count()}\")\n\n# Force CPU device globally\ntorch.set_default_device('cpu')\ndevice = torch.device('cpu')\nprint(f\"‚úÖ Default device set to: {device}\")\n\n# Import transformers and other libraries\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\nfrom datasets import Dataset\n\nprint(\"üìö All libraries imported successfully\")\nprint(\"üéØ Ready for CPU-only fine-tuning!\")",
   "id": "807d86a2a4037fd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cell 3: Create SUPER simple training data\n",
    "# Goal: Teach the model to complete crypto-related sentences"
   ],
   "id": "e9085a54b4f10cfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_data = [\n",
    "    \"When Bitcoin price goes up, the market sentiment is bullish.\",\n",
    "    \"If a project has a rug pull, investors will lose money.\",\n",
    "    \"High trading volume usually indicates strong market interest.\",\n",
    "    \"When fear dominates the market, prices tend to drop.\",\n",
    "    \"A successful token launch often leads to price appreciation.\",\n",
    "    \"Security audits are important for smart contract safety.\",\n",
    "    \"Market capitalization reflects the total value of a cryptocurrency.\",\n",
    "    \"Liquidity pools enable decentralized trading on DEXs.\",\n",
    "    \"Whale movements can significantly impact token prices.\",\n",
    "    \"Technical analysis helps predict short-term price movements.\"\n",
    "]\n",
    "\n",
    "# Convert to dataset format\n",
    "dataset = Dataset.from_dict({\"text\": training_data})\n",
    "print(f\"üìà Training samples: {len(dataset)}\")"
   ],
   "id": "bb0aab42a5bcdab7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 4: Tokenize the data",
   "id": "91180b490ff89949"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,  # Keep it short for quick training\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(\"‚úÖ Data tokenized!\")"
   ],
   "id": "8c843fa8ba4223f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 5: Set up training arguments (MINIMAL for quick testing)",
   "id": "fab592c1219cd061"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# STEP 5: CPU-Only Training Arguments (Anti-CUDA Configuration)\n\nprint(\"üîß Configuring training arguments for strict CPU-only operation...\")\n\n# Disable Accelerate library CUDA detection\nimport accelerate\nfrom accelerate import Accelerator\n\n# Force Accelerator to use CPU only\naccelerator = Accelerator(cpu=True)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./models/fine_tuned/hello_world\",\n    num_train_epochs=2,           # Reduced for CPU training\n    per_device_train_batch_size=1, # Small batch for CPU\n    gradient_accumulation_steps=2, # Compensate for smaller batch size\n    logging_steps=1,              # See progress immediately\n    save_steps=50,                # Less frequent saves\n    \n    # CPU-ONLY SETTINGS - CRITICAL FOR GTX 1050\n    remove_unused_columns=False,\n    fp16=False,                   # No mixed precision on CPU\n    bf16=False,                   # No bfloat16 on CPU  \n    dataloader_pin_memory=False,  # No GPU memory pinning\n    report_to=[],                 # Disable wandb/tensorboard logging\n    no_cuda=True,                 # Explicitly disable CUDA\n    use_cpu=True,                 # Force CPU usage\n    \n    # DISABLE ACCELERATE OPTIMIZATIONS\n    disable_tqdm=False,           # Keep progress bars\n    dataloader_num_workers=0,     # No multiprocessing (can cause CUDA issues)\n    skip_memory_metrics=True,     # Skip GPU memory metrics\n    \n    # FORCE CPU DEVICE\n    local_rank=-1,                # No distributed training\n    device='cpu',                 # Explicit CPU device\n)\n\nprint(\"‚úÖ Training arguments configured:\")\nprint(f\"   üìä Epochs: {training_args.num_train_epochs}\")\nprint(f\"   üî¢ Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"   üö´ CUDA disabled: {training_args.no_cuda}\")\nprint(f\"   üñ•Ô∏è  CPU enforced: {getattr(training_args, 'use_cpu', 'Not set')}\")\nprint(f\"   üíæ FP16 disabled: {not training_args.fp16}\")\nprint(\"   ‚ö° Accelerator configured for CPU-only operation\")",
   "id": "74d95ecb0d7f49f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 6: Create trainer",
   "id": "5d14be081ce11774"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# STEP 6: Create CPU-Only Trainer with Device Verification\n\nprint(\"üéØ Creating trainer with comprehensive device verification...\")\n\n# Final device verification before trainer creation\nprint(\"\\nüîç Pre-trainer device verification:\")\nprint(f\"   üñ•Ô∏è  Model device: {next(model.parameters()).device}\")\nprint(f\"   üìä PyTorch default device: {torch.tensor(1.0).device}\")\nprint(f\"   üö´ CUDA available: {torch.cuda.is_available()}\")\nprint(f\"   üì± CUDA devices: {torch.cuda.device_count()}\")\n\n# Ensure all data will be on CPU by checking tokenized dataset\nsample_batch = tokenized_dataset[0]\nif 'input_ids' in sample_batch:\n    sample_tensor = torch.tensor(sample_batch['input_ids'])\n    print(f\"   üìù Sample data device: {sample_tensor.device}\")\n\n# Create trainer with explicit CPU-only configuration\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n)\n\nprint(\"\\n‚úÖ Trainer created successfully!\")\n\n# Post-creation verification\nprint(\"\\nüîç Post-trainer device verification:\")\nprint(f\"   üñ•Ô∏è  Trainer model device: {trainer.model.device}\")\nprint(f\"   üìä Training dataset size: {len(trainer.train_dataset)}\")\n\n# Check if trainer is using accelerate and its configuration\nif hasattr(trainer, 'accelerator'):\n    print(f\"   ‚ö° Accelerator device: {trainer.accelerator.device}\")\n    print(f\"   üö´ Accelerator CPU mode: {trainer.accelerator.cpu}\")\nelse:\n    print(\"   ‚ö° No accelerator detected\")\n\n# Final safety check - scan for any CUDA tensors\nprint(\"\\nüõ°Ô∏è  Final safety check...\")\ncuda_found = False\nfor name, param in model.named_parameters():\n    if param.device.type == 'cuda':\n        print(f\"   ‚ùå CUDA tensor found: {name}\")\n        cuda_found = True\n\nif not cuda_found:\n    print(\"   ‚úÖ All tensors confirmed on CPU\")\n    print(\"   üéØ Ready for safe CPU training!\")\nelse:\n    print(\"   ‚ö†Ô∏è  CUDA tensors detected - training may fail\")\n\nprint(f\"\\nüöÄ Trainer ready for CPU-only training on GTX 1050 system!\")",
   "id": "a8d4b401fb33d0d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 7: Train! (This should take 1-3 minutes)",
   "id": "b28bdc69d5550573"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cell 8: Test the fine-tuned model",
   "id": "ea06c3e4f4ccaf8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# STEP 7: Execute CPU Training\n\nprint(\"üèãÔ∏è Starting CPU training...\")\nprint(\"‚è∞ This will take a few minutes on CPU - please be patient\")\n\n# One final verification before training\nprint(f\"\\nüìä Pre-training status:\")\nprint(f\"   Model device: {next(trainer.model.parameters()).device}\")\nprint(f\"   CUDA available: {torch.cuda.is_available()}\")\nprint(f\"   Training epochs: {trainer.args.num_train_epochs}\")\nprint(f\"   Batch size: {trainer.args.per_device_train_batch_size}\")\n\ntry:\n    # Start training\n    trainer.train()\n    \n    print(\"\\nüéâ Training completed successfully!\")\n    print(\"üíæ Model saved to: ./models/fine_tuned/hello_world\")\n    print(\"‚úÖ GTX 1050 compatibility issue resolved!\")\n    \nexcept Exception as e:\n    print(f\"\\n‚ùå Training failed with error: {e}\")\n    print(\"üìã Error type:\", type(e).__name__)\n    \n    # If it's still a CUDA error, provide guidance\n    if \"CUDA\" in str(e):\n        print(\"\\nüîç CUDA error detected even with CPU-only setup.\")\n        print(\"üí° Suggested solutions:\")\n        print(\"   1. Ensure you restarted the kernel after Step 1\")\n        print(\"   2. Check that CUDA_VISIBLE_DEVICES is empty\")\n        print(\"   3. Try completely restarting Jupyter\")\n        print(f\"   4. Current CUDA_VISIBLE_DEVICES: '{os.environ.get('CUDA_VISIBLE_DEVICES', 'NOT_SET')}'\")\n    \n    raise e",
   "id": "3bfcfad69cbae361",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# STEP 8: Test the Fine-Tuned Model (CPU-Only)\n\nprint(\"üß™ Testing fine-tuned model with CPU-only inference...\")\n\n# Verify model is still on CPU after training\nprint(f\"üîç Model device verification: {next(model.parameters()).device}\")\n\ntest_prompts = [\n    \"When Ethereum price goes up,\",\n    \"If a project has good fundamentals,\", \n    \"High trading volume indicates\"\n]\n\nprint(f\"\\nüéØ Running {len(test_prompts)} test completions:\")\n\nfor i, prompt in enumerate(test_prompts, 1):\n    print(f\"\\n--- Test {i}/{len(test_prompts)} ---\")\n    \n    # Tokenize input (ensure it's on CPU)\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    # Verify input tensors are on CPU\n    input_device = inputs.input_ids.device\n    print(f\"Input device: {input_device}\")\n    \n    if input_device.type == 'cuda':\n        print(\"‚ö†Ô∏è  Input on CUDA - moving to CPU\")\n        inputs = {k: v.to('cpu') for k, v in inputs.items()}\n    \n    # Generate text with CPU-optimized settings\n    try:\n        with torch.no_grad():  # Save memory during inference\n            outputs = model.generate(\n                inputs.input_ids,\n                attention_mask=inputs.attention_mask,\n                max_length=30,\n                num_return_sequences=1,\n                temperature=0.7,\n                pad_token_id=tokenizer.eos_token_id,\n                do_sample=True,\n                top_p=0.9,\n                device='cpu'  # Explicit CPU device for generation\n            )\n        \n        # Decode result\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(f\"‚úÖ Prompt: {prompt}\")\n        print(f\"üìù Completion: {generated_text}\")\n        \n    except Exception as e:\n        print(f\"‚ùå Generation failed: {e}\")\n        if \"CUDA\" in str(e):\n            print(\"   Still encountering CUDA issues during inference\")\n        \n    print(\"-\" * 60)\n\nprint(\"\\nüéâ Testing completed!\")\nprint(\"‚úÖ Your fine-tuned model is working on CPU\")\nprint(\"üñ•Ô∏è  GTX 1050 compatibility achieved!\")\nprint(\"\\nüí° The model is now specialized for crypto-related text generation\")",
   "id": "111f995fbeadaab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b60fe23811fb9515",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
